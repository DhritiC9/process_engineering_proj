  0%|                                                 | 0/12500 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  4%|█▍                                   | 500/12500 [09:55<3:48:32,  1.14s/it]/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 2.6504, 'grad_norm': 21129.619140625, 'learning_rate': 4.8004e-05, 'epoch': 0.4}
  warnings.warn(                                                                
{'eval_loss': 0.08020597696304321, 'eval_runtime': 45.0179, 'eval_samples_per_second': 22.213, 'eval_steps_per_second': 1.399, 'epoch': 0.4}
  8%|██▉                                 | 1000/12500 [20:43<3:51:57,  1.21s/it]/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.0979, 'grad_norm': 17512.619140625, 'learning_rate': 4.6004e-05, 'epoch': 0.8}
  warnings.warn(                                                                
{'eval_loss': 0.047494981437921524, 'eval_runtime': 46.5981, 'eval_samples_per_second': 21.46, 'eval_steps_per_second': 1.352, 'epoch': 0.8}
 12%|████▎                               | 1500/12500 [31:32<3:36:35,  1.18s/it]/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.0666, 'grad_norm': 13786.1455078125, 'learning_rate': 4.4004e-05, 'epoch': 1.2}
  warnings.warn(                                                                
{'eval_loss': 0.03765640780329704, 'eval_runtime': 45.7571, 'eval_samples_per_second': 21.855, 'eval_steps_per_second': 1.377, 'epoch': 1.2}
 16%|█████▊                              | 2000/12500 [42:23<3:21:30,  1.15s/it]/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
{'loss': 0.0533, 'grad_norm': 8857.4755859375, 'learning_rate': 4.2004000000000006e-05, 'epoch': 1.6}
  warnings.warn(                                                                
{'eval_loss': 0.030900750309228897, 'eval_runtime': 45.8276, 'eval_samples_per_second': 21.821, 'eval_steps_per_second': 1.375, 'epoch': 1.6}
 17%|██████▏                             | 2163/12500 [45:55<3:19:50,  1.16s/it]Traceback (most recent call last):
  File "/home/dhriti/final.py", line 83, in <module>
    trainer.train()
  File "/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/transformers/trainer.py", line 2240, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/transformers/trainer.py", line 3791, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/accelerate/accelerator.py", line 2469, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/dhriti/.conda/envs/byt5/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
